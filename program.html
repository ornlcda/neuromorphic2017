<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">
    <link rel="shortcut icon" href="../../assets/ico/favicon.ico">

    <title>Neuromorphic Computing Workshop - Program</title>

    <!-- Bootstrap core CSS -->
    <link href="./dist/css/bootstrap.min.css" rel="stylesheet">

    <!-- Just for debugging purposes. Don't actually copy this line! -->
    <!--[if lt IE 9]><script src="../../assets/js/ie8-responsive-file-warning.js"></script><![endif]-->

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Custom styles for this template -->
    <link href="rrcf2014.css" rel="stylesheet">
  </head>
<!-- NAVBAR
================================================== -->
  <body>
    <div class="navbar-wrapper">
      <div class="container">

        <div class="navbar navbar-inverse navbar-static-top" role="navigation">
          <div class="container">

            <div class="navbar-header">
              <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
              </button>
                       	<a class="brand" href="index.html"> <img src="images/ncw-logo.png" alt="Neuromorphic Computing Workshop: Architectures, Models, Environments, and Applications
"></a>
              <!--	<a class="brand" href="index.html"> <img src="images/mlhpc.png" alt=""Rapid Response Cyber Forensics"MLHPC2015"></a>-->
            </div>

            <div class="navbar-collapse collapse pull-right">
              <ul class="nav navbar-nav">
                <li class="active"><a href="index.html">Home</a></li>
<li><a href="https://www.ornl.gov/content/come-see-us">Venue</a></li>
                <li><a href="index.html#important-dates">Important Dates</a></li>
                <li class="dropdown">
                    <a href="#" class="dropdown-toggle" data-toggle="dropdown">Information<b class="caret"></b></a>
                    <ul class="dropdown-menu">
                        <li><a href="registration.html">Registration and Hotel</a></li>

                        <li><a href="cfp.html">Call for Papers</a></li>
                        <li><a href="keynotespeakers.html">Keynotes</a></li>
                        <li><a href="program.html">Program</a></li>
                    <!-- <li><a href="files/ORNLNeuromorphicComputingWorkshop2016Report.pdf">Report PDF</a></li>-->
                    </ul>
                </li>
                <li class="dropdown">
          			<a href="#" class="dropdown-toggle" data-toggle="dropdown">Committees<b class="caret"></b></a>
          			<ul class="dropdown-menu">
            			<li><a href="organizingcommittee.html">Organizing Committee</a></li>
            			<li><a href="programcommittee.html">Program Committee</a></li>
            		</ul>
        		</li>
                <li><a href="index.html#contact">Contact</a></li>
              </ul>
            </div>
          </div>
        </div>

      </div>
    </div>


    <!-- Marketing messaging and featurettes
    ================================================== -->
    <!-- Wrap the rest of the page in another container to center all the content. -->

    <div class="container marketing">

      <!-- START THE FEATURETTES -->
      
      

      <hr class="featurette-divider">

      <div class="row featurette">
        <div class="col-md-7">

          <h2 class="featurette-heading">Workshop <span class="text-muted">Program</span></h2>
          <hr class="featurette-divider">
                  <!--
          <p class="lead">Wednesday, June 29, 2016</p>
          <ul>
          <li>10:30-11:30 - Check-In/Badge Pick-Up (ORNL Visitor's Center)</li>
          <li>11:00-12:30 - Welcome/Working Lunch
          <ul>
          <li>Shaun Gleason, ORNL <a href="presentations/ShaunGleasonORNL&CCSDNeuromorphicWorkshop.pdf">(presentation)</a></li>
          <li>Robinson Pino, DOE ASCR <a href="presentations/Pino_DOE_SC_ASCR_Neuromorphic_v1.pdf">(presentation)</a></li>
          <li>Tom Potok, ORNL</li>
          </ul>
          </li>
          <li>12:30-1:15 - Keynote: Todd Hylton, Brain Corporation <a href="presentations/Hylton-ORNLNeuromorphicComputingtalk-June2016.pdf">(presentation)</a></li>
          <li>1:15-2:00 - Keynote: Catherine Schuman, Oak Ridge National Laboratory <a href="presentations/KatieSchumanKeynote.pdf">(presentation)</a></li>
          <li>2:00-2:45 - Keynote: Cindy Leiton, Stony Brook University <a href="presentations/NCW_slides_final_cindy_leiton.pdf">(presentation)</a></li>
          <li>2:45-3:30 - Break</li>
          <li>3:30-4:00 - Presentation: Lloyd Whitman, White House Office of Science and Technology Policy <a href="presentations/Whitman-DOE-Neuromorphic-GrandChallenge-2016-06-29-handouts.pdf">(presentation)</a></li>
          <li>4:00-5:00 - Focus Area Overview: Architectures/Models, Algorithms, Applications</li>
          <li>5:00-5:30 - Summary</li>
          </ul>
          <hr class="featurette-divider">
          <p class="lead">Thursday, June 30, 2016</p>
          <ul>
          <li>8:00-8:30 - Coffee/Light Breakfast</li>
          <li>8:30-8:40 - Welcome/Recap</li>
          <li>8:40-10:00 - Short Presentations (20 minutes)
          <ul>
          <li>James Aimone, Kristofor Carlson and Fredrick Rothganger: <a href="presentations/Aimone_NeuralComputingScaleComplexity-OakRidge.pdf">Neural Computing: What Scale and Complexity is Needed?</a></li>
          <li>Alice Parker: <a href="presentations/Parker_OakRidge_Final.pptx">Object Recognition and Learning using the BioRC Biomimetic Real-Time Cortex Neurons</a></li>
          <li>Kathleen Hamilton, Alexander McCaskey, Jonathan Schrock, Neena Imam and Travis Humble: <a href="presentations/Associative_Memory_Models_with_Adiabatic_Quantum_Computation.pdf">Associative Memory Models with Adiabatic Quantum Optimization</a></li>
          <li>Tinoosh Mohsenin and Farinaz Koushanfar: <a href="presentations/DOE2016-mohsenin-koushanfar.pdf">Bringing Physical Dimensions to the Deep Networks for Neuromorphic Computing</a></li>
          </ul>
          <li>10:00-10:30 - Break</li>
          <li>10:30-12:00 - Short Presentations (20 minutes)
          <ul>
          <li>Yuan Xie: Architecture, ISA support, and Software Toolchain for Neuromorphic Computing in ReRAM Based Main Memory</li>
          <li>Angel Yanguas-Gil: <a href="presentations/angel_yanguas-gil_neuro.pdf">Beyond the crossbar: materials based design and emulation of neuromemristive devices and architectures</a></li>
          <li>Matthew J. Marinella, Sapan Agarwal, A. Alec Talin, Conrad D. James and F. Rick McCormick: Device to System Modeling Framework to Enable a 10 fJ per Instruction Neuromorphic Computer</li>
          <li>Chris Carothers, Noah Wolfe, Prasanna Date, Mark Plagge, and Jim Hendler: <a href="presentations/Wolfe-Large-Scale_Hybrid_Neuromorphic_HPC_Simulations_Algorithms_and_Applications.pdf">Large-Scale Hybrid Neuromorphic HPC Simulations, Algorithms and Applications</a></li>
          </ul>
          <li>12:00-1:00 - Lunch and <a href="presentations/stanwilliams_doe_neuromorphic_workshop_july.pptx">Plenary by Stan Williams</a>.</li> 
          <li>1:00-2:30 - Short Presentations (20 minutes)
          <ul>
          <li>Priyadarshini Panda and Kaushik Roy: Enabling on-chip intelligence with low-power neuromorphic computing</li>
          <li>Yu Cao, Steven Skorheim, Ming Tu, Pai-Yu Chen, Shimeng Yu, Jae-Sun Seo, Visar Berisha, Maxim Bazhenov and Zihan Xu: <a href="presentations/Yu-Cao-RHINO-v2.pdf">Efficient Neuromorphic Learning with Motifs of Feedforward Inhibition</a></li>
          <li>Praveen Pilly, Nigel Stepp and Jose Cruz-Albrecht: <a href="presentations/ORNL_HRLs_Neuromorphic_redux.pdf">Exploiting Criticality in HRL's Latigo Neuromorphic Device</a></li>
          <li>Yiran Chen: <a href="presentations/YiranChen_ORNL2016.pdf">Algorithm Innovations of Enhancing Scalability and Adaptability of Learning Systems</a></li>
          </ul>
          <li>2:30-3:00 - Break</li>
          <li>3:00-3:45 - Lightning Talks (10 minutes)</li>
          <ul>
          <li>Vishal Saxena and Xinyu Wu: <a href="presentations/Oak_Ridge_Presentation_June_16_Public_Release_vishal_saxena.pdf">Addressing Challenges in Neuromorphic Computing with Memristive Synapses</a></li>
          <li>Tarek Taha, Raqibul Hasan and Chris Yakopcic: <a href="presentations/taha_dayton_nca.pdf">Energy Efficiency and Throughput of Multicore Memristor Crossbar Based Neuromorphic Architectures</a></li>
          <li>Dhireesha Kudithipudi, James Mnatzaganian, Anvesh Polepalli, Nicholas Soures, and Cory Merkel: <a href="presentations/Kudithipudi-ORNL-Neuromorphic-Workshop-2016.pdf">Energy Efficient and Scalable Neuromemristive Computing Substrates</a></li>
          <li>Gangotree Chakma, Elvis Offor, Mark Dean and Garrett Rose: <a href="presentations/GRose_mrDANNA_NCAMA16.pdf">A Reconfigurable Memristive DANNA Circuit with Implementations in Pattern Recognition</a></li>
          </ul>
          <li>3:45-5:00 - Presenter Panel Discussion, moderated by Mark Dean</li>
          <li>5:00-5:30 - Summary</li>
          </ul>
          <hr class="featurette-divider">
          <p class="lead">Friday, July 1, 2016</p>
          <ul>
          <li>7:30-8:00 - Coffee/Light Breakfast</li>
          <li>8:00-10:00 - Breakout Discussion Sessions
		<ul>
		<li>Workshop Report Content Discussions</li>
		<li>Example DOE Workshop Report: <a href="http://science.energy.gov/~/media/bes/pdf/reports/2016/NCFMtSA_rpt.pdf">Neuromorphic Computing: From Materials to Systems Architecture</a></li>
		</ul> 
	</li>
          <li>10:00-10:30 - Break</li>
          <li>10:30-11:45 - Breakout Discussion Sessions
		<ul>
		<li>Workshop Report Content Discussions</li>
		<li>Example DOE Workshop Report: <a href="http://science.energy.gov/~/media/bes/pdf/reports/2016/NCFMtSA_rpt.pdf">Neuromorphic Computing: From Materials to Systems Architecture</a></li>
		</ul> 
          <li>11:45-12:00 - Final Wrap-Up</li>
          </ul>
	</li>
	-->

          <!--<p class="lead">Location: Oak Ridge National Laboratory Conference Center</p>-->
          <p class="lead">Date: July 17, 2017 - July 19, 2016 </p>
          <p class="lead"> Schedule: TBA </p>
          <!--<p class="lead">Time: 2:00pm - 5:30pm</p>-->
        </div>
        <div class="col-md-5">
          <!-- <img class="featurette-image img-responsive" data-src="holder.js/500x500/auto" alt="Generic placeholder image"> -->
        </div>
      </div>
      
    <!--  <hr class="featurette-divider">-->
      

      <div class="row featurette">
     <!--   
        
        <div class="col-md-7">
          <h2 class="lead">Keynote: Bryan Catanzaro<br>
          2:00pm - 3:00pm</h2>
          <p class="lead"><span class="text-muted">Baidu Research Silicon Valley Artificial Intelligence Laboratory</span></p>
          <p>During the past few years, deep learning has made incredible progress towards solving many previously difficult Artificial Intelligence (AI) tasks.  Although the techniques behind deep learning have been studied for decades, they rely on large datasets and large computational resources, and so have only recently become practical for many problems.  Training deep neural networks is very computationally intensive: training one model takes tens of exaflops of work, and so HPC techniques are key to creating these models.  As in other fields, progress in AI is iterative, building on previous ideas.  This means that the turnaround time in training models is a key bottleneck to progress in AI—the quicker an idea can be realized as a trainable model, train it on a large dataset, and test it, the quicker that ways can be found of improving the models.  In this talk, Catanzaro will discuss the key insights that make deep learning work for many problems, describe the training problem, and detail the use of standard HPC techniques that allow him to rapidly iterate on his models.  He will explain how HPC ideas are becoming increasingly central to progress in AI and will also show several examples of how deep learning is helping solve difficult AI problems.</p>
        </div>
        -->
        
        <!--<div class="col-md-5">-->
          <!--<img class="featurette-image img-responsive" src="bio_images/Catanzaro.jpg" alt="Bryan Catanzaro">-->
          <!--
        </div>
        -->
      </div>
      
     <!-- <hr class="featurette-divider">-->
      <div class="row featurette">
        
       <!-- 
        <div class="col-md-7">
          <h2 class="lead">Coffee Break<br>
          3:00pm - 3:30pm</h2>
        </div>
      </div>

      <hr class="featurette-divider">

      <div class="row featurette">

        <div class="col-md-7">
          <h2 class="lead">Asynchronous Parallel Stochastic Gradient Descent - A Numeric Core for Scalable Distributed Machine Learning Algorithms<br>
          3:30pm - 3:55pm</h2>
          <p class="lead"><span class="text-muted">Janis Keuper and Franz-Josef Pfreundt</h2></span></p>
          <p>The implementation of a vast majority of machine learning (ML) algorithms boils down
to solving a numerical optimization problem. In this context, Stochastic
Gradient Descent (SGD) methods have long proven to provide good results, both
in terms of convergence and accuracy. Recently, several parallelization approaches
have been proposed in order to scale SGD to solve very large ML problems.
At their core, most of these approaches are following a MapReduce scheme.
This paper presents a novel parallel updating algorithm for SGD, which utilizes
the asynchronous single-sided communication paradigm.
Compared to existing methods, Asynchronous Parallel Stochastic Gradient Descent (ASGD) provides faster convergence,
at linear scalability and stable accuracy.</p>
        </div>
        
      </div>
      
      <hr class="featurette-divider">

      <div class="row featurette">

        <div class="col-md-7">
          <h2 class="lead">HPDBSCAN – Highly Parallel DBSCAN<br>
          3:55pm - 4:20pm</h2>
          <p class="lead"><span class="text-muted">Markus Götz, Christian Bodenstein and Morris Riedel</h2></span></p>
          <p>Clustering algorithms in the field of data-mining are used
to aggregate similar objects into common groups. One of
the best-known of these algorithms is called DBSCAN. Its
distinct design enables the search for an apriori unknown
number of arbitrarily shaped clusters, and at the same time
allows to filter out noise. Due to its sequential formulation, the parallelization of DBSCAN renders a challenge. In
this paper we present a new parallel approach which we call
HPDBSCAN. It employs three major techniques in order
to break the sequentiality, empower workload-balancing as
well as speed up neighborhood searches in distributed parallel processing environments i) a computation split heuristic
for domain decomposition, ii) a data index preprocessing
step and iii) a rule-based cluster merging scheme.
As a proof-of-concept we implemented HPDBSCAN as an
OpenMP/MPI hybrid application. Using real-world data
sets, such as a point cloud from the old town of Bremen,
Germany, we demonstrate that our implementation is able
to achieve a significant speed-up and scale-up in common
HPC setups. Moreover, we compare our approach with previous attempts to parallelize DBSCAN showing an order of
magnitude improvement in terms of computation time and
memory consumption.</p>
        </div>
        
      </div>
      
      <hr class="featurette-divider">

      <div class="row featurette">

        <div class="col-md-7">
          <h2 class="lead">LBANN: Livermore Big Artificial Neural Network HPC Toolkit<br>
          4:20pm - 4:45</h2>
          <p class="lead"><span class="text-muted">Brian Van Essen, Hyojin Kim, Roger Pearce, Kofi Boakye and Barry Chen</span></p>
          <p>Recent successes of deep learning have been largely driven by the ability to train large models on vast amounts of data. We believe that High Performance Computing (HPC) will play an increasingly important role in helping deep learning achieve the next level of innovation fueled by neural network models that are orders of magnitude larger and trained on commensurately more training data. We are targeting the unique capabilities of both current and upcoming HPC sys- tems to train massive neural networks and are developing the Livermore Big Artificial Neural Network (LBANN) toolkit to exploit both model and data parallelism optimized for large scale HPC resources. This paper presents our prelimi- nary results in scaling the size of model that can be trained with the LBANN toolkit.</p>
        </div>
        
      </div>
      
      <hr class="featurette-divider">

      <div class="row featurette">

        <div class="col-md-7">
          <h2 class="lead">Optimizing Deep Learning Hyper-Parameters Through an Evolutionary Algorithm<br>
          4:45pm - 5:10pm</h2>
          <p class="lead"><span class="text-muted">Steven Young, Derek Rose, Thomas Karnowski, Seung-Hwan Lim and Robert Patton</h2></span></p>
          <p>There has been a recent surge of success in utilizing Deep Learning (DL) in imaging and speech applications for its relatively automatic feature generation and, in particular for convolutional neural networks (CNNs), high accuracy classification abilities. While these models learn their parameters through data-driven methods, model selection (as architecture construction) through hyper-parameter choices remains a tedious and highly intuition driven task. To address this, Multi-node Evolutionary Neural Networks for Deep Learning (MENNDL) is proposed as a method for automating network selection on computational clusters through hyper-parameter optimization performed via genetic algorithms.</p>
        </div>
        
      </div>
	


      
      <hr class="featurette-divider">

      <div class="row featurette">

        <div class="col-md-7">
          <h2 class="lead">Dynamic Adaptive Neural Network Arrays: A Neuromorphic Architecture<br>
          5:10pm - 5:30pm</h2>
          <p class="lead"><span class="text-muted">Catherine Schuman, Adam Disney and John Reynolds</h2></span></p>
          <p>Dynamic Adaptive Neural Network Array (DANNA) is a
neuromorphic hardware implementation. It differs from most
other neuromorphic projects in that it allows for programmability of structure, and it is trained or designed using evo-
lutionary optimization. This paper describes the DANNA
structure, how DANNA is trained using evolutionary optimization, and an application of DANNA to a very simple
classification task.</p>
        </div>
        
      </div>
      
      
      
      -->
      
      <hr class="featurette-divider">
	
	  <!--
      <hr class="featurette-divider">
Program
	
      <div class="row featurette">
        <div class="col-md-7">
          <h2 class="featurette-heading">And lastly, this one. <span class="text-muted">Checkmate.</span></h2>
          <p class="lead">Donec ullamcorper nulla non metus auctor fringilla. Vestibulum id ligula porta felis euismod semper. Praesent commodo cursus magna, vel scelerisque nisl consectetur. Fusce dapibus, tellus ac cursus commodo.</p>
        </div>
        <div class="col-md-5">
          <img class="featurette-image img-responsive" data-src="holder.js/500x500/auto" alt="Generic placeholder image">
        </div>
      </div>

      <hr class="featurette-divider">
      -->

      <!-- /END THE FEATURETTES -->


      <!-- FOOTER -->
      <footer>
      	<div style="width: 100%;overflow:auto;">
          <div style="float:left; width: 50%">

            <a name="contact"></a>
            <p><b>Contact:  Thomas Potok</b>, potokte "at" ornl.gov</p>
            <p>&copy; 2015 Oak Ridge National Laboratory</p>
          </div>
          <!--
          <div style="float:right;">
            <p>In cooperation with</p>
            <a class="brand" href="http://www.sighpc.org/"> <img src="images/sighpc_logo_72dpi.jpg" alt="Machine Learning in HPC Environments"></a>
          </div>
          -->
        </div>
        <p class="pull-right"><a href="#">Back to top</a></p>
      </footer>

    </div><!-- /.container -->


    <!-- Bootstrap core JavaScript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.0/jquery.min.js"></script>
    <script src="./dist/js/bootstrap.min.js"></script>
    <script src="./assets/js/docs.min.js"></script>
  </body>
</html>
